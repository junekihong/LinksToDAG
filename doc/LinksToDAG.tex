%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage[small]{caption}
\usepackage{longtable}

\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{soul}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{tikz-dependency}

\newcommand{\eqnref}[1]{\eqref{eqn:#1}}

\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}  % allows better color names
\usepackage[disable]{todonotes}   % insert [disable] to disable all notes
% Note that these macros accept optional arguments such as 
% [size=\small,bordercolor=red].
\newcommand{\Note}[4][]{\todo[author=#2,color=#3,fancyline,#1]{#4}}
\newcommand{\noteJH}[2][]{\Note[#1]{JH}{blue!40}{#2}}
\newcommand{\noteJE}[2][]{\Note[#1]{JE}{green!40}{#2}}   
\newcommand{\notewho}[3][]{\Note[#1]{#2}{orange!40}{#3}}  % extra arg with miscellaneous author
\newcommand{\NoteJH}[2][]{\noteJH[inline,#1]{#2}}
\newcommand{\NoteJE}[2][]{\noteJE[inline,#1]{#2}}
\newcommand{\Notewho}[3][]{\notewho[inline,#1]{#2}{#3}}  % extra arg with miscellaneous author

% \newcommand{\root}{\texttt{\$}}


\title{Deriving Multi-Headed Projective Dependency Parses \\ from Link Grammar Parses}
% Oriented link grammar: Creating a multi-headed dependency corpus.

% \author{Juneki Hong and Jason Eisner\\
%   Department of Computer Science \\
%   Johns Hopkins University \\
%   Baltimore, MD 21218, USA \\ 
%   {\tt \{juneki,jason\}@cs.jhu.edu} \\
% }

\date{}

\setlength\titlebox{3cm}    % Expanding the titlebox

\begin{document}
\maketitle

\begin{abstract}
Under multi-headed dependency grammar, a parse is a directed acyclic graph rather than a tree.  Such formalisms can be more syntactically and semantically expressive.  However, 
it is hard to train, test, or improve multi-headed parsers because few multi-headed corpora exist, particularly for the projective case.
% OLD
% investigate the benefit of such parsers or to work on making them faster or more accurate.  
To help fill this gap, we observe that link grammar already produces {\em undirected} projective graphs.  
% OLD
% ... produces parses that are similar to multi-headed projective dependency parses, except that the links are undirected. 
We use Integer Linear Programming to assign consistent directions to the labeled links in a corpus of several thousand parses produced by the Link Grammar Parser, which has broad-coverage hand-written grammars of English as well as Russian and other languages.  We find that such directions can indeed be consistently assigned in a way that yields valid multi-headed dependency parses. The resulting parses in English appear reasonably linguistically plausible, though differing in style from CoNLL-style parses of the same sentences; we discuss the differences.  
\end{abstract}

\section{Multi-Headed Dependency Parsing}

Dependency parsing maps a sentence to a directed graph whose vertices are the words $1, 2, \ldots, n$ of the sentence along with a distinguished ``root'' vertex 0.  A labeled directed edge $u \stackrel{L}{\rightarrow} v$ or $v \stackrel{L}{\leftarrow} u$ indicates that the ``child'' $v$ is some kind of argument or modifier of its ``parent'' $u$.  The edge label $L$ indicates the specific syntactic or semantic relationship between the two words.  

In the special case $u=0$, the edge designates $v$ as playing some special top-level role in the sentence, e.g., as the main verb.  We disallow $v=0$.

As recently discussed by \newcite{gomezrodriguez-nivre-2013}, one might impose various requirements on the parse graph:
\begin{itemize}[noitemsep]
\item {\sc Single-Head}: each word has $\leq 1$ parent
\item {\sc Acyclic}: there are no directed cycles
\item {\sc Connected}: each pair of words has a undirected path between them
\item {\sc Reachable}: each word can be reached from 0 by a directed path ($\Rightarrow$ {\sc Connected})
\item {\sc Planar}: edges may not ``cross''\footnote{That is, if there are edges between $i,j$ and between $u,v$, where $i < u < j$, then {\sc Planar} requires $i \leq v \leq j$.}
\end{itemize}
It is common to impose all of these requirements at once, leading to a {\em projective dependency parser} that produces projective trees rooted at 0.\footnote{{\sc Projective} basically means {\sc Planar} + {\sc Reachable}.  Note: It does not prevent 0 from having multiple children.}  However, parsing algorithms can be devised that relax any or all of the requirements \cite{gomezrodriguez-nivre-2013}.  

In this paper, we are interested in relaxing the {\sc Single-Head} requirement while preserving all the others.  This means that the parse can have more than $n$ edges, allowing it to express more relationships between words.  In English, for example, here are some constructions that seem to call for a multi-headed analysis.  
\begin{description}
\item[control] In {\em ``Jill likes to skip,''} the word {\em Jill} is the subject of two verbs.  In {\em ``Jill persuaded Jack to skip,''} {\em Jack} is the object of one verb and the subject of another.  Without recognizing this, our parser would miss the syntactic invariants that {\em skip} always has a subject and {\em persuaded} always has an object.  It would also be unable to exploit the selectional preferences of both verbs to help disambiguate the parse.  This is why we prefer to make the parser aware of multi-headedness, rather than using a single-headed parser and then extracting the additional semantic roles from its output.
\item[relativization] In {\em ``The boy that Jill skipped with fell down,''} the word {\em boy} is the object of {\em with} as well as the subject of {\em fell}.  Without recognizing this, we would miss the syntactic invariant that {\em with} always has an object.  
\item[conjunction] In {\em ``Jack and Jill went up the hill,''} {\em Jack} and {\em Jill} serve as the two arguments to {\em and}, but they are also semantically subjects of {\em went}.  Without recognizing this, we would have no (local) reason for expecting the arguments of {\em and} to be nouns.
\end{description}

In linguistics, it is common to analyze some of these structures using trees with ``empty categories.''  The subject of {\em skip} is taken to be a silent morpheme {\em PRO}:
{\em ``Jill$_i$ likes PRO$_i$ to skip.''}  However, this is no longer a tree if one considers the implicit undirected edge between {\em Jill} and {\em PRO} (denoted by their shared index $i$).  Our simpler representation contracts this coreference edge, eliminating {\em PRO} and creating a {\em Jill} $\leftarrow$ {\em skip} link.  

\section{Link Grammars}

A few past NLP papers have explored multi-headed dependency parsing \cite{buchkromann-2006,mcdonald-pereira-2006,sagae-tsujii-2008,gomezrodriguez-nivre-2013}.  Unfortunately, there seem to be no annotated corpora in this form other than the Danish Dependency Treebank \cite{kromann-2003}; researchers have sometimes converted corpora from other formats such as HPSG.  

% ADDRESSED BY ABOVE?
% \NoteJE{We should acknowledge that another option would be to post-process either CoNLL parses or our link parses to automatically add more multi-heading, e.g., to handle control phenomena ("John wants to skip" or "Jane ran for office in order to change the tax laws" should have John/Jane be the subject of two verbs).  However, that requires per-language}

All of these options result in non-projective parses, so the parsers must use non-projective or pseudo-projective algorithms.

It seems at first that no one has worked out annotation conventions for {\em projective} multi-headed dependency parsing.  However, this is not quite true.  Link Grammar \cite{SleatorTemperly91} is a grammar-based formalism for projective dependency parsing with {\em undirected} links.  It produces undirected connected planar graphs.  Annotation conventions are implicit in the detailed lexicon for the Link Grammar Parser,\noteJE{check caps}%
\footnote{\url{http://www.abisource.com/projects/link-grammar/dict/introduction.html}.  The 122 link types in the English lexicon are documented at \url{http://www.abisource.com/projects/link-grammar/dict/summarize-links.html}.} 
which specifies for every word a constraint on the {\em sequence} of labeled leftward and rightward edges attached to it.  As remarked by \newcite{eisner-2000-iwptbook}, this is analogous to dependency grammar's use of head automata to constrain a word's sequence of left and right children.  For example, in {\em ``The boy that Jill skipped with fell down,''} the word {\em with} uses a lexical entry that requires it to link to a governing verb to the left, an extracted object farther to the left, and nothing to the right.  Each entry has a hand-assigned cost in \{0,1,2\} and the parser finds the parse of minimum total cost.
% Information about cost is in this message from Linas Vepstas on 2014-02-01: 
%   https://groups.google.com/d/msg/link-grammar/eeJw1Ofgc9U/diqPYSwuFfoJ

Given a link grammar parse, it would be straightforward to convert it to an acyclic dependency parse by orienting all edges rightward.  However, the result may violate the {\sc Reachable} constraint.  Instead we could orient all edges by depth-first search from the root node, which yields a DAG satisfying all our constraints.  However, this might result in inconsistent annotation conventions, with some \texttt{S}-labeled links pointing from subject to verb and others from verb to subject.  

We supposed that the link grammar lexicon designers actually had a consistent direction in mind for each edge type.  We would expect verbs to point to their subject arguments in dependency grammar, and so we surmise that all \texttt{S} links are intended to point leftward (from verb to subject: {\em ``Jack $\stackrel{\texttt{S}}{\leftarrow}$ is falling''}).  The link grammar designers take care to use a distinct \texttt{SI} label in cases of subject-verb inversion, and we surmise that \texttt{SI} links are intended to point rightward (again from verb to subject: {\em ``Is $\stackrel{\texttt{SI}}{\rightarrow}$ Jack falling?''}).

Our goal in this paper is to recover these implicit directions by global optimization.  We seek a fixed mapping from labels to directions such that link grammar parses become directed dependency parses that satisfy all of our constraints.

Our first thought was to seek a direction mapping such that no parsed word sequence allowed by the link grammar lexicon could possibly violate our constraints after directions were imposed.  This is a well-defined constraint programming problem.  For example, to prevent cyclicity, we would require (roughly speaking) that no word type in the lexicon could follow a sequence of directed rightward links through other word types and then a leftward link back to itself.  

However, working directly with the link grammar lexicon format is somewhat tricky.  We also feared that there would not be a perfect solution---for example, because of errors in the lexicon, or linguistically unnatural word sequences not anticipated by the grammar designers.  In this case it would not be clear how to relax our constraints.

Thus, we chose to use a sample of {\em actual} sentences parsed by the link grammar, and to seek a direction mapping so that {\em these} parses would not violate our constraints after directions were imposed.  If no such mapping exists, then we are willing to orient a few edge tokens in the wrong direction to ensure that the parses are still well-formed---but we minimize the number of such violations.  In this way, the empirical distribution of sentences guides our assignment of directions.  The resulting directed corpus can be used for research on multi-headed dependency parsing.

% Link grammars are a grammatical system equivalent to context-free grammars that assign linking requirements to every given word. A link parser then tries to satisfy all of these requirements for every word of a sentence while still maintaining planarity. The resulting links describe the relationships between constituents in a parse. 

% The link grammar is based on a set of handwritten dictionaries. Instead of going through these dictionaries, we learned the grammar using an ILP. This approach also potentially allows us to analyze link grammar dictionaries other than English. \todo{explore other link grammar dictionaries} 

% \NoteJE{what statistics does it use?  what languages are available}

% Link parsing in contrast produces a multiheaded planar graph with undirected edges, where every edge has a label describing the relationship between two constituents in a parse. In this paper we explore whether these relationships include dependencies, and whether the multiheadedness of the link grammar offers additional dependency relationships not found in other corpora.

% Dependency parsing is the task of mapping a sentence to a projective (not always projective?) directed acyclic tree. Link parsing in contrast produces a multiheaded planar graph with undirected edges, where every edge has a label describing the relationship between two constituents in a parse. In this paper we explore whether these relationships include dependencies. To determine the directional dependencies within the link edge labels we will use integer linear programming, encoding the problem in the Zimpl little language \cite{Koch2004}. It turns out that the link parses roughly only match half of the conll dependency corpus. However this is because \todo{}. 

\section{Data Sets}
We used the English sentences from the CoNLL 2007 Shared Task \cite{CONLL-SHARED-2007}---a subset of the Penn Treebank for which {\em single}-headed reference parses are available. 
%We also used a prefix of the Russian National Corpus,\footnote{That is, a prefix when considering the files in Unix sort order by filename.  We dropped sentences with $\leq 2$ words, as well as file 22592319, which appeared to contain charts.} which is unparsed.
We also used a prefix of the Russian News Commentary data from the ACL 2013 Shared Task of Machine Translation,\footnote{\url{http://www.statmt.org/wmt13/training-monolingual-nc-v8.tgz}} which is unparsed.

We generated link parses using the AbiWord/CMU link grammar parser version 5.0.8 \cite{LINKPARSER-2014}.  The parser's coverage is not perfect: we obtained connected parses for only \input{figure/conll_analysis_sentences_total.tex}(of \input{figure/conll_analysis_sentences_original.tex}$\!\!$) English sentences and only \input{figure/russian_sentence_count.tex}(of \input{figure/russian_original_sentence_count.tex}$\!\!$) Russian sentences, discarding the other sentences.
These two languages have the most mature lexicons at present, although lexicons for 7 other languages are available.  

The link grammar parses do differ in style from the single-headed CoNLL parses of the same English sentences.  They have \input{figure/conll_analysis_edge_percent.tex}more edges overall.  Only \input{figure/conll_analysis_precision.tex}of the links match CoNLL arcs, and only \input{figure/conll_analysis_recall.tex}of the CoNLL arcs match links.
% DATA FOR THE ABOVE
% \begin{figure}[ht!]
%   \centering
%   \small
%   \input{figure/conll_analysis_arcs.tex}
%   \caption{Arcs that match links}
% \end{figure}
% 
% OLD
% Using the sentences of the dependency corpus as comparison, we find that the link parses do not wholly subsume dependency parses, and that the undirected links match roughly three fourths of the arcs in the conll dependency corpus. \todo{This is because...} 


% With the English sentences we compared our link parse results to the original dependency annotations, and with the Russian sentences we only produced link parses.

%\subsection{Discarding Incomplete Link Parses}

%\NoteJE{ideally we could cut this section given what I wrote above.  However, you seem to be saying that we did train on sentences with dropped words as long as the parses were connected.  Didn't we decide to drop those sentences too?}

%Not all of the data was used for this work. From the given sentences, we discarded those that the link parser could not process and returned a parse with nodes disconnected from the root. \input{figure/conll_analysis_sentences_disconnected} of the English sentences were removed in this way. The ILP was run only on the connected parses to produce a corpus of directionalized link parses. 

%For our analysis of these parses, we further discarded those that had dropped words, where the link parser could not attach links to every word in the sentence. About \input{figure/conll_analysis_sentences_dropped} of the non-disconnected sentences were removed, leaving us working only with complete link parses.

%\begin{figure}[ht!]
%  \centering
%  \small
%  \input{figure/conll_analysis_sentences.tex}
%  \caption{\small The English sentences used for this work.}
%\end{figure}

%\noteJH{The stability results come from the second row of this table. The analysis later in the paper uses the third row. Putting this section here might be confusing to readers?}

\section{Integer Linear Programming Model}

For each undirected labeled edge $ij$ in the link corpus, where $i,j$ denote tokens in the same sentence with $i < j$, we introduce nonnegative integer variables $x_{ij}$ and $x_{ji}$ with a constraint $x_{ij}+x_{ji}=1$.  We interpret $x_{ij}=1$ or $x_{ji}=1$ to mean that the link has direction $i \rightarrow j$ or $i \leftarrow j$, respectively.\footnote{In practice we halve the number of variables by replacing $x_{ji}$ with $1-x_{ij}$ for $j > i$, but that obscures the exposition.}

% OLD VERSION
% we introduce a variable $x_{ij}$ that is 0 or 1 according to whether $e$ points left or right.  We abbreviate $1-x_{ij}$ as $\bar{x}_{ij}$.

For each non-0 token $v$, we ensure that it has at least one parent by constraining\footnote{To denote two linked tokens, we use variables $i,j$ when $i$ is to the left of $j$, or variables $u,v$ when $u$ is the parent of $v$.}
\begin{align}\label{eqn:oneparent}
\sum_u x_{uv} & \geq 1
\end{align}
% OLD VERSION
% $$\sum_{u < v} x_{uv} + \sum_{u > v} \bar{x}_{vu} \geq 1$$
where $u$ ranges only over tokens such that the relevant variable exists.
%
To prevent cycles,\footnote{This also ensures {\sc reachable}, given \eqnref{oneparent}.} for each token $v$ we introduce a depth variable $d_v$ in the range $[0,n_v]$ (not constrained to be integer), where $n_v$ is the length of the sentence containing $v$.  We require a child's depth to be at least 1 greater than each of its parents' depths---constraints that can be satisfied iff the sentence has no directed cycles:
\begin{align}\label{eqn:nocycles}
(\forall u)\; d_v + (1+n_v) \cdot (1-x_{uv}) & \geq 1+d_u
% OLD VERSION
% $$(\forall u < v) d_v + (1+n_v) \cdot \bar{x}_{uv} \geq 1+d_u$$
% $$(\forall u > v) d_v + (1+n_v) \cdot x_{vu} \geq 1+d_u$$
\end{align}
The second summand ensures that \eqnref{nocycles} is trivially satisfied (hence has no effect) when $u$ is {\em not} the parent of $v$.  
\noteJE{in final version: figure out whether it is faster to require the 0 tokens to have depth 0, or add all the depths (perhaps multiplied by 0.001) to the objective function to break ties in simplex, or neither.  See also other notes in tex file.}
% As a speedup, we can require the 0 tokens to have depth 0. \noteJE{does this really still make a difference to speed?  
% \noteJE{what's the point of the root\_depth constraint in the .zpl file?  Almost trivially satisfied.}

Finally, we encourage all links with the same label \noteJE{explain that we mean capital letters?} to have the same direction.  For each label $L$, we introduce binary variables $r_L$ and $\ell_L$, which say whether a link of type $L$ is ``allowed'' to point right or left, respectively.  For each undirected edge $ij$ of label $L$, with $i < j$, we write
\begin{align}
x_{ij} &\leq r_L + s_{ij} & 
x_{ji} &\leq \ell_L + s_{ij}
\end{align}
where $s_{ij} \geq 0$ is a slack variable that allows an edge token to point in a disallowed direction if needed to ensure \eqnref{oneparent}--\eqnref{nocycles}. 

Our objective tries to minimize the number of allowed directions (by link type---cf.\@ \newcite{ravi2009}) and the total slack (by link token):
\begin{align}\label{eqn:obj}
\min \left( \sum_{L} r_L + \ell_L \right) \cdot \frac{N_L}{4} + \sum_{ij} s_{ij}
\end{align}
where $N_L$ is the number of link tokens with label $L$.  Objective \eqnref{obj} is willing to tolerate up to 1/4 of those link tokens' using a disallowed direction before it prefers to allow {\em both} directions. \noteJE{check!}
% CUT FOR SPACE
% (i.e., $r_L+\ell_L=2$).  

\NoteJE{It might be a good idea to have the second allowed direction still carry some lesser slack penalty, so that we break ties in favor of the preferred direction.  The two bits for each label would then say ``which direction is preferred?'' and ``is the other direction allowed?''}

% OLD
% We formulate an ILP to find the minimal set of link type to directionality assignments that would directionalize all the links in a set of link grammar parses, while still mandating that all of the resulting dependency parses would be connected DAGs, and with all the nodes reachable from the root. Our task of finding the smallest set of assignments that would explain the parses is also called model minimization\cite{ravi2009}. \noteJE{worth citing, but not quite the same thing}
% For every encountered label we generate two boolean label/direction variables (e.g. label/left, label/right). Setting one of these variables to \textsc{true} allows the label type to only go in the specified direction, while setting both would allow the label to go in either way.
% The objective is to minimize the number of label/direction variables set to \textsc{true}, while still satisfying acyclicity and reachability constraints. The encoding was written in the Zimpl little language \cite{Koch2004} and solved using the SCIP Optimization Suite \cite{achterberg2009scip}.
% \subsection{Slack}
% We introduced slack such that the link types were allowed to deviate from the majority one percent of the time before the ILP would assign both label/direction variables to \textsc{true}. This allows for flexibility against noise in the link parser's label assignments, balanced with allowing for both directions to be assigned if necessary.

\section{Experiments and Results}
\begin{figure*}[ht!]
  \vspace{-6pt}
  \centering
  \input{figure/sentences.tikz}
  \caption{\small Example Parses: The blue upper edges show CoNLL dependency parses; the red lower edges show oriented link grammar parses. Vertical edges are those with parent 0.  See Appendix~\ref{app:parses} for formatting details and more examples.}
  \label{fig:parses}
  \vspace{-6pt}
\end{figure*}


We solved our ILP problem using the SCIP Optimization Suite \cite{achterberg2009scip}, encoding it using the ZIMPL language \cite{Koch2004}.  Our largest run took 1.5 hours. \noteJH{A run of 10,000 sentences takes 3142 seconds (update in final version!)}  Detailed results from this run are included as Appendix~\ref{app:linktypes}.  On English, only \input{figure/allowed_labels_both.tex}of \input{figure/allowed_labels_total.tex}link {\em types} allowed both directions, and only $\sum_{ij} s_{ij} = \input{figure/label_token_disallowed}$ of \input{figure/label_token_total.tex}link {\em tokens} required a disallowed direction via slack.
\input{figure/conll_analysis_sentences_multiheaded.tex}of the English sentences but only \input{figure/russian_multiheaded.tex}of the Russian ones had at least one multiheaded word.
%\noteJE{2 significant digits is enough}
%\noteJE{update these numbers}



\subsection{Stability of Results}\label{sec:stability}
\begin{figure}[ht!]
  \small
  \centering
  \includegraphics[width=\linewidth, keepaspectratio=true]{figure/precision_recall.png}
  \caption{Convergence to the direction mapping obtained on the largest run.  Precision relative to that run is very high even for small datasets.  Recall grows as more link types are seen.}\label{fig:convergence}
\vspace{-6pt}
\end{figure}

We worried that the direction mapping might be unstable and sensitive to the input corpus.  Figure~\ref{fig:convergence} shows otherwise (for both English and Russian). \noteJE{IMPORTANT! current graph is with fine-grained tags; rerun with the coarse-grained tags as in the rest of the paper} Using even a small prefix of the data got very high-precision results, in the sense that nearly all $r_L$ or $\ell_L$ variables that were 1 in this lightly trained mapping were also 1 in our largest run.  The only disadvantage to using small data is low recall relative to the large run---many of the labels $L$ are not observed yet and so we do not yet allow either direction ($r_L=\ell_L=0$).  

We used only coarse link tags as our labels, keeping only the capital letters of a tag (and merging all \texttt{ID} tags).  This is because other characters in a tag indicate fine-grained features such as plurality that generally do not affect link direction.  However, when we tried using fine tags as our labels instead, we found that all refinements of the same coarse tag would almost always spontaneously agree on their preferred direction.  This indicates that there is indeed a ``natural'' direction for the coarse tag and that we can find it.

% OLD
% We report on the convergence of the results of our ILP with increasing data. With increasing trials of sentences, we measure how similar the subsequent direction mappings would be with each other. Taking the direction table of the largest run as the answer key, we compared how much the solutions of the smaller runs deviated from it. 
% We recorded the precision of whether the mappings in the smaller runs could be found in those of the largest run, and similarly the recall of whether the mappings in the largest run could be found in the smaller runs. 

% Because the precision values were high, we found that the solutions of the previous runs were consistent with the largest run, and as the data increased so did the recall values, and so we would continue to encounter novel link types and then incorporate them.

\subsection{Linguistic Analysis}

The resulting English corpus uses a syntactic annotation scheme that is somewhat different from the CoNLL annotations.  Differences are tabulated in Appendix~\ref{app:linktypes}, and the actual parses are contrasted in Figure~\ref{fig:parses} and Appendix~\ref{app:parses}.

The link grammar results in multi-headed treatments of infinitivals, compound determiners, relative clauses, and embedding.  The other annotation differences are generally reasonable, e.g., letting {\em 's} be the head of a possessive, and different handling of punctuation and lists.  Of course, a few of the differences are due to parser attachment errors.

The main vexation is the handling of subject-verb links.  Under the English link grammar, the verb (or 0) that governs a clause will link to both the clause's subject and its last (main) verb.  This would permit our desired treatment of {\em ``Jill persuaded him to skip''}, in which {\em ``him''} has two parents.  But the ILP solution generally treats subjects as {\em parents} of verbs (thus we get {\em him} $\rightarrow$ {\em to}), owing to inconsistencies in the link grammar itself.\footnote{Why?  When the subject is missing ({\em ``Jill wanted to skip''}), the governing verb now links instead to the clause's first (tensed) verb but {\em no longer} to the last as well.  That means that a VP must be headed by its first verb ({\em to} $\rightarrow$ {\em skip}).  As a result, when the subject is present, it must be the {\em parent} of the first verb ({\em him} $\rightarrow$ {\em to} $\rightarrow$ {\em skip}), since no other parent is available.  ({\em Note:} Subjects are not {\em always} parents.  The construction {\em ``It was impossible, they said''} is handled inconsistently by the link grammar:
the subject ({\em ``they''}) links only to the verb ({\em ``said''}) and so must be its child, no other parent being available.)}


% Multiheaded:
% IV (890/891) - multiheaded treatment of infinitivals
% I (1783/2969) - multiheaded treatment of infinitivals
% B (795/842) - leftward extraction from relative clauses
% CV (1588/1799) - multiheaded treatment of subordination
% AL (39/40) - compound determiners
% PP (448/689) - subject attaches to aux as well as main verb
% HA (3/3) -
% JQ (2/2) - 
% SF (85/111) - pleonastic subject
% AF (6/12) - 
% C (244/1813) - 
% OF (155/291)
% P (651/2060) - 
% Q (12/30)- 
% RS (300/305) - 
% S (4256/7434) -
% SX (4/6) - 
% W (2321/4721) - 
% WV (1800/4138) - 

\NoteJE{what do we say about Russian?  ask Ryan for final version?}


% DOESN'T SEEM NECESSARY FOR THE LANGUAGES WE TRIED
% \section{Future Work}
% \subsection{Slack Hierarchy}
% We would like to introduce slack on the link grammar types such that link types with the same coarse grained label would try and align the same way as the majority in the group, where the preceding capital letters of the link type denote the coarse grained label, while the subscript letters denote further information. This slack would place a prior on rare or never-before seen link types to be assigned in the same way as other similar link types. We think that this will give the ILP better flexibility to handle noise and novel link types while still trying to learn the overall link grammar.

\section{Conclusions}

We have presented an automatic ILP-based method to ``orient'' link grammar parses in multiple languages, turning them into rooted connected DAGs.  This improves their linguistic interpretability and provides new corpora for experimenting with multi-headed dependency parsers.  

More generally, ILP may be a valuable technology for enriching existing annotated corpora.  For example, the Penn Treebank \cite{TREEBANK-1993} omitted types of annotations that plausibly could be reconstructed by rule.

\onecolumn
\section{Sample of Appendix}
For further explanations, the complete table of English Link Types, and an additional 100 example English parses, see \url{http://tidyurl.com/LinksToDAG_supplement}.


\subsection{Sample of English Link Types}

\renewcommand{\arraystretch}{1.2}    % more space between rows
\input{figure/sample_link_analysis_coarse_table.tex}


\subsection{Sample of English Parses}

\begin{figure*}[ht!]
  \vspace{-6pt}
  \centering
  \input{figure/sample_parses.tikz}
  \vspace{-6pt}
\end{figure*}




\twocolumn

% \newpage
\bibliographystyle{acl2014}
\bibliography{LinksToDAG}


\appendix
\clearpage
\onecolumn

% OLD
% \section{ILP Formulation}
% We present our ILP formulation. 
% 
% \begin{figure}[!htb]
%   \small
%   \begin{align*}
%     \text{Sets:}&&\\
%     &\textsc{link}_{\textsc{sentence},\textsc{label}, i,j} \in \textsc{links} &\\
%     &\textsc{label} \in \textsc{labels}&\\ 
%     &\textsc{dir} = \{\textsc{left}, \textsc{right}\} &\\
%     \text{Params:}&&\\
%     &\textsc{m}_\textsc{length} = \max (\forall{\textsc{link}_{\textsc{sentence}, i,j}} \max(i,j)) &\\
%     %&\textsc{count}_{\textsc{label}, \textsc{dir}} = \sum_{\textsc{link}_{\textsc{label},\textsc{dir}}}1 &\\
%     &\textsc{cost}_{\textsc{label}} = \frac{100}{ \sum_{\textsc{link}_{\textsc{label},\textsc{dir}}}1 } &\\
%     \text{Variables:}&&\\
%     &\textsc{slack}_{\textsc{link}} \geq 0 &\\ 
%     &\textsc{depth}_{\textsc{sentence},i} \geq 0 &\\
%     &\textsc{allowed}_{\textsc{label},\textsc{dir}} = \{0,1\}&\\
%     &\textsc{arc}_{\textsc{link}} = \{0,1\} &\\
%     \text{minimize:}& &\\ 
%     &\sum_{\textsc{label}, \textsc{dir}} (\textsc{allowed}_{\textsc{label},\textsc{dir}}) + \textsc{cost}_{\textsc{label}} \cdot \sum_\textsc{link} \textsc{slack}_{\textsc{link}} &\\
%     &&\\
%     \text{subject to: }& &\\
%     &\text{Links go in the allowed directions, except for slack}&\\
%     &\forall{\textsc{link}_{i,j,\textsc{label}}}: 1-\textsc{arc}_{\textsc{link}} \leq \textsc{allowed}_{\textsc{label}, \textsc{left}} + \textsc{slack}_{\textsc{link}} &\\
%     &\forall{\textsc{link}_{i,j,\textsc{label}}}: \textsc{arc}_{\textsc{link}} \leq \textsc{allowed}_{\textsc{label}, \textsc{right}} + \textsc{slack}_{\textsc{link}} &\\
%     &\text{Depth constraints for acyclicity}&\\
%     &\forall{\textsc{link}_{\textsc{sentence},i}}: \textsc{depth}_{\textsc{sentence},\text{root}} \leq \textsc{depth}_{\textsc{sentence},i} &\\
%     &\forall{\textsc{link}_{\textsc{sentence},i,j}}: \textsc{depth}_{\textsc{sentence},i} + \textsc{m}_\textsc{length}\cdot\textsc{arc}_{\textsc{sentence},i,j} \geq \textsc{depth}_{\textsc{sentence},j} + 1 &\\
%     &\forall{\textsc{link}_{\textsc{sentence},i,j}}: \textsc{depth}_{\textsc{sentence},j} + \textsc{m}_\textsc{length}\cdot(1-\textsc{arc}_{\textsc{sentence},i,j}) \geq \textsc{depth}_{\textsc{sentence},i} + 1 &\\
%     &\text{Every token has a parent}&\\
%     &\forall{\textsc{link}_{\textsc{sentence},i \neq \textsc{root}}}: \sum_{\textsc{link}_{\textsc{sentence},i,j}} (1-\textsc{arc}_{\textsc{sentence},i,j}) + \sum_{\textsc{link}_{\textsc{sentence},j,i}}(\textsc{arc}_{\textsc{sentence},j,i}) \geq 1&\\
%   \end{align*}
%   \caption{\small The ILP formulation.}
% \end{figure}
% 
% 
% \clearpage

\section{English Link Types}\label{app:linktypes}

Recall that \url{http://www.abisource.com/projects/link-grammar/dict/summarize-links.html} documents the 122 link types in the English link grammar lexicon.  Below is the result of our largest English run, whose corpus uses \input{figure/allowed_labels_total.tex}of these types. \noteJH{automatically generate this number correctly: 107}

Other researchers could postprocess link grammar parses by using the first two columns.  These indicate whether a given link label $L$ tended to be oriented rightward or leftward by our ILP.  

The labels that do not have a consistent direction are mainly those that connect a conjunction to its conjuncts.\footnote{See \url{http://www.abisource.com/projects/link-grammar/dict/coordination.html}.}  
Our coarsening of the labels (see section~\ref{sec:stability}) discarded the characters that distinguish left from right conjuncts.

The third column indicates where multi-headedness happens, by showing how often the child of an $L$ link had additional parents as well.

The remaining columns compare to the CoNLL parses.  They show the fraction of $L$-labeled link tokens that were also present in the CoNLL reference parse---and on that fraction, whether the orientation matches CoNLL's and what the dominant CoNLL label was.

%\NoteJE{improve table formatting.  Numeric justification; en dashes; longtable; remove most hlines.}

\renewcommand{\arraystretch}{1.2}    % more space between rows
\input{figure/link_analysis_coarse_table.tex}

\section{Sample English Parses}\label{app:parses}

\NoteJE{show the fine-grained link labels in the parses, even though the ILP deals with coarse-grained.  Juneki tried already to hack this in but was getting double links.}

Below we display the first 100 English sentences whose widths do not exceed the width of this page.  The blue upper half shows the CoNLL tagging and dependency parse.  The red lower half shows the link grammar's tagging and our orientation of the link parse.  
% The dashes in the link grammar part of speech tag annotations mean that the link parser did not return one.
A vertical edge to word token $v$ denotes an edge $0 \rightarrow v$.
Words that were unknown to the link parser are flagged with \texttt{[!]}.  

Edges are shown as solid lines if they appear in both parses.  They are drawn more thickly if the directions do not match.

Edges are shown as dotted lines if they appear only in one parse and not in the other.  Such edges are highlighted in orange if the child has multiple parents.  

% OLD NERDVIEW EXPLANATION
% For every arc:
% \begin{enumerate}
% \item If there is a corresponding link between the same nodes then a solid blue line is drawn.
% \item Or else a dotted blue line is drawn. 
% \end{enumerate}
% The bottom red arcs represent the oriented link grammar parses. For every link edge:
% \begin{enumerate}
% \item If there is a corresponding CoNLL arc between the same nodes then a solid red line is drawn.
% \item If the matching arc is facing the opposite direction, then the link edge is drawn thicker.
% \item Of the remaining edges, if an edge is a multiheaded link, then it is drawn with thick orange.
% \item The rest of the remaining edges are drawn with dotted red lines.
% \end{enumerate}

% Uncomment this to add in all of the parses. Warning: compilation will take a long time.
\input{figure/parses.tikz}


\end{document}
